{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBtUsXLWbc1RJgfr6GenNc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lillanieder/data-projects/blob/main/2_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "s_7kH2RldkbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"tweet_data.csv\")\n",
        "tweets = [tweet.lower() for tweet in df.tweet_retweet.drop_duplicates().to_list()]"
      ],
      "metadata": {
        "id": "uObnMz9bdjcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "FhZ0yPpzdssJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Creating token\n",
        "# [[token for token in tweet.split()] for tweet in tweets] \n",
        "raw_tokenized_docs = [TweetTokenizer().tokenize(doc) for doc in tweets]\n",
        "\n",
        "# Preprocessing tokens: eliminating stopwords and punctuation\n",
        "# from gensim.utils import simple_preprocess\n",
        "# tokenized_docs = [simple_preprocess(str(doc), deacc=True) for doc in raw_tokenized_docs]\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['via'])\n",
        "tokenized_docs =\\\n",
        "[[word.lower() for word in doc \n",
        "  if word not in stop_words and\n",
        "     word not in string.punctuation+'...' and\n",
        "     'â€¢' not in word]\n",
        " for doc in raw_tokenized_docs]"
      ],
      "metadata": {
        "id": "SoYlqhkidjel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09c82cb-de3d-4292-b956-a403dfdb9e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "enKhTFpwjU2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "\n",
        "# Creating dictionary and the bag-of-words (Corpus)\n",
        "dictionary = Dictionary(tokenized_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "# Create the defaultdict: total_word_count\n",
        "total_word_count = defaultdict(int)\n",
        "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
        "    total_word_count[dictionary.get(word_id)] += word_count\n",
        "\n",
        "# Create a sorted list from the defaultdict: sorted_word_count \n",
        "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
        "\n",
        "# Print the top 5 words across all documents alongside the count\n",
        "for word, word_count in sorted_word_count[:10]:\n",
        "    print(word, word_count)"
      ],
      "metadata": {
        "id": "PuPDYmb5P0nH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c4b80f-6e71-4777-807a-7e170998e430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "climate 3128\n",
            "change 3027\n",
            "global 2822\n",
            "warming 2764\n",
            "rt 902\n",
            "link 751\n",
            "new 310\n",
            "#climate 224\n",
            "snow 223\n",
            "#tcot 222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "bigram = gensim.models.Phrases(tokenized_docs, min_count=10, threshold=5)\n",
        "\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "\n",
        "bigrams = [bigram_mod[doc] for doc in tokenized_docs]\n",
        "bi_dictionary = Dictionary(bigrams)\n",
        "bi_corpus = [bi_dictionary.doc2bow(doc) for doc in bigrams]\n",
        "\n",
        "# Create the defaultdict: total_word_count\n",
        "total_word_count = defaultdict(int)\n",
        "for word_id, word_count in itertools.chain.from_iterable(bi_corpus):\n",
        "    total_word_count[word_id] += word_count\n",
        "\n",
        "# Create a sorted list from the defaultdict: sorted_word_count \n",
        "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
        "\n",
        "# Print the top 5 words across all documents alongside the count\n",
        "for word_id, word_count in sorted_word_count[:20]:\n",
        "    print(bi_dictionary.get(word_id), word_count)"
      ],
      "metadata": {
        "id": "f04-MJ095WJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce5edfa-e098-4427-9461-472316b30ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "climate_change 2624\n",
            "global_warming 2477\n",
            "rt 873\n",
            "link 751\n",
            "climate 330\n",
            "change 211\n",
            "new 203\n",
            "warming 189\n",
            "snow 185\n",
            "science 179\n",
            "bill 169\n",
            "#tcot 162\n",
            "us 160\n",
            "global 157\n",
            "report 149\n",
            "news 148\n",
            "energy 147\n",
            "#climate_change 146\n",
            "a_ 136\n",
            "dc 122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "\n",
        "lda_model = LdaModel(corpus=bi_corpus, id2word=bi_dictionary, num_topics=5, random_state=1, \n",
        "                     update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
        "\n",
        "pprint(lda_model.print_topics())"
      ],
      "metadata": {
        "id": "u4o9KhAhCgHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40396239-c35f-4bed-b536-8a2ff217cb93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.036*\"global_warming\" + 0.010*\"weather\" + 0.010*\"un\" + 0.010*\"snow\" + '\n",
            "  '0.008*\"volcanoes\" + 0.008*\"think\" + 0.006*\"times\" + 0.006*\"would\" + '\n",
            "  '0.006*\"cause\" + 0.006*\"make\"'),\n",
            " (1,\n",
            "  '0.113*\"global_warming\" + 0.037*\"rt\" + 0.014*\"climate\" + 0.014*\"change\" + '\n",
            "  '0.012*\"may\" + 0.011*\"new\" + 0.011*\"birds\" + 0.011*\"bats\" + 0.011*\"#tcot\" + '\n",
            "  '0.011*\"lizards\"'),\n",
            " (2,\n",
            "  '0.015*\"science\" + 0.012*\"bill\" + 0.011*\"scientists\" + 0.011*\"ice\" + '\n",
            "  '0.009*\"global_warming\" + 0.009*\"one\" + 0.007*\"get\" + 0.007*\"obama\" + '\n",
            "  '0.007*\"hoax\" + 0.006*\"5\"'),\n",
            " (3,\n",
            "  '0.047*\"climate_change\" + 0.016*\"global\" + 0.009*\"important\" + 0.008*\"says\" '\n",
            "  '+ 0.007*\"questions\" + 0.006*\"people\" + 0.005*\"@newsongreen\" + 0.005*\"first\" '\n",
            "  '+ 0.005*\"like\" + 0.005*\"earth_day\"'),\n",
            " (4,\n",
            "  '0.132*\"climate_change\" + 0.010*\"news\" + 0.010*\"energy\" + 0.009*\"us\" + '\n",
            "  '0.008*\"green\" + 0.008*\"report\" + 0.007*\"warming\" + 0.006*\"role\" + '\n",
            "  '0.006*\"say\" + 0.006*\"cl\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import TfidfModel\n",
        "\n",
        "tfidf = TfidfModel(bi_corpus, smartirs='ntc')\n",
        "\n",
        "tfidf_weights = defaultdict(float)\n",
        "for doc in tfidf[bi_corpus]:\n",
        "  for id, freq in doc:\n",
        "    tfidf_weights[bi_dictionary[id]] = round(freq,4)\n",
        "\n",
        "sorted_tfidf_weights = sorted(tfidf_weights.items(), key=lambda w: w[1], reverse=False)\n",
        "sorted_tfidf_weights[:20]"
      ],
      "metadata": {
        "id": "3a6HoMW2UUz5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c48acca-318d-4ebc-928b-9e0730625ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('global_warming', 0.0344),\n",
              " ('climate_change', 0.0376),\n",
              " ('link', 0.0821),\n",
              " ('rt', 0.1022),\n",
              " ('change', 0.1215),\n",
              " ('climate', 0.122),\n",
              " ('report', 0.1416),\n",
              " ('dc', 0.1461),\n",
              " ('science', 0.1493),\n",
              " ('u', 0.15),\n",
              " ('press', 0.1533),\n",
              " ('us', 0.1548),\n",
              " ('warming', 0.1579),\n",
              " ('talk', 0.1623),\n",
              " ('a_', 0.1685),\n",
              " ('global', 0.1696),\n",
              " ('energy', 0.1708),\n",
              " ('fight_climate', 0.1717),\n",
              " ('world', 0.1733),\n",
              " ('#tcot', 0.174)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uRaodC0IUU2_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}